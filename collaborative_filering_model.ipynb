{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce9f9f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a91ac908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Id  Count\n",
      "0  609   5162\n",
      "1  249   4931\n",
      "2   76   4928\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------question1-------------------------------\n",
    "\n",
    "d = pd.read_csv('Tags.csv')\n",
    "df = pd.read_csv('Posts.csv')\n",
    "\n",
    "df_Q = df.loc[df['PostTypeId']==1]\n",
    "\n",
    "tags_df = d[['Id','TagName']].copy()\n",
    "tags_df['Count']=0\n",
    "\n",
    "def process_tags(tags_string, tag_name):\n",
    " \n",
    "    tags_list = tags_string.strip('|').split('|')\n",
    "\n",
    "    for tag in tags_list:\n",
    "        tag.strip()\n",
    "\n",
    "    return tag_name in tags_list\n",
    "\n",
    "for i,row in d.iterrows():\n",
    "\n",
    "    tag_name = row['TagName']\n",
    "\n",
    "    if tag_name:\n",
    "        count = df_Q['Tags'].apply(lambda x: process_tags(x,tag_name)).sum()\n",
    "        tags_df.at[i,'Count']=count\n",
    "           \n",
    "tags_df = tags_df[['Id','Count']]\n",
    "sorted_df = tags_df.sort_values(by='Count',ascending=False)\n",
    "Tags = sorted_df.reset_index(drop=True)\n",
    "top_tag_3 = Tags.loc[0:2, ['Id', 'Count']]\n",
    "print(top_tag_3)\n",
    "\n",
    "\n",
    "df1=pd.read_csv('Posts.csv')\n",
    "df1_A=df1.loc[df1['PostTypeId']==2]\n",
    "df1_AU=df1_A[['OwnerUserId','ParentId']]\n",
    "Answerers = df1_AU[['OwnerUserId','ParentId']].value_counts().reset_index().rename(columns={\"OwnerUserId\": \"OwnerUserId\", \"ParentId\": \"ParentId\", \"count\": \"Count\"})\n",
    "Answerers = Answerers[['OwnerUserId']]\n",
    "# Answerers = Answerers.value_counts().reset_index().rename(columns={\"OwnerUserId\": \"Id\", \"count\": \"Count\"})\n",
    "# top_answerer_3 = Answerers.loc[0:2,['Id','Count']]\n",
    "# print(top_answerer_3)\n",
    "# print(Answerers.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64ad5398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Id     0\n",
      "0    9113.0  2838\n",
      "1  177980.0  2318\n",
      "2    1204.0  2042\n",
      "Index(['Id', 0], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "Answerers = Answerers.value_counts().reset_index().rename(columns={\"OwnerUserId\": \"Id\", \"count\": \"Count\"})\n",
    "top_answerer_3 = Answerers.loc[0:2,['Id',0]]\n",
    "print(top_answerer_3)\n",
    "print(Answerers.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "254cd45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the Expert Matrix: (1160, 973)\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------question2--------------------------------\n",
    "\n",
    "# experts = Answerers.loc[Answerers['Count']>=20]\n",
    "experts = Answerers.loc[Answerers[0]>=20]\n",
    "experts = experts.sort_values(by='Id',ascending=True)\n",
    "\n",
    "tags = Tags.loc[Tags['Count']>=20]\n",
    "tags = tags.sort_values(by='Id',ascending=True)\n",
    "\n",
    "Posts_filtered = df1_A.loc[df1_A['OwnerUserId'].isin(experts['Id'])]\n",
    "ParentId_posts= df_Q.loc[df_Q['Id'].isin(Posts_filtered['ParentId'])]\n",
    "\n",
    "tags_filtered = d.loc[d['Id'].isin(tags['Id'])]\n",
    "tag_identifier=tags_filtered.set_index('Id')['TagName'].to_dict()\n",
    "\n",
    "#expert matrix\n",
    "dfm = pd.DataFrame(index=experts['Id'], columns=tags['Id'])\n",
    "\n",
    "for e in experts['Id']:\n",
    "    exp_post_ans=Posts_filtered.loc[Posts_filtered['OwnerUserId']==e]\n",
    "    que_to_exp_ans=ParentId_posts.loc[ParentId_posts['Id'].isin(exp_post_ans['ParentId'])]\n",
    "\n",
    "    for t in tags['Id']:\n",
    "        tag_name= str(tag_identifier.get(t))\n",
    "        if tag_name:\n",
    "            count = que_to_exp_ans['Tags'].apply(lambda x: process_tags(x, tag_name)).sum()\n",
    "            if(count!=0):dfm.loc[e,t]=count\n",
    "\n",
    "print(f\"Dimension of the Expert Matrix: {dfm.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35b2d4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summation of the Utility Matrix: 41180.0\n",
      "Highest Row Sum: 1162.0\n",
      "Highest Column Sum: 1403.0\n",
      "Summation of train matrix: 32047.0\n",
      "Dimension of test matrix: (174, 146)\n",
      "Summation Value of test matrix: 642.0\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------question3-------------------------------\n",
    "\n",
    "def function(value):\n",
    "    if pd.isna(value):\n",
    "        return value\n",
    "    if value<15:\n",
    "        value = math.floor(value/3)\n",
    "    else:\n",
    "        value = 5\n",
    "    return value\n",
    "\n",
    "#utility matrix\n",
    "dfm1=dfm.apply(lambda x: x.map(function))\n",
    "\n",
    "dfm1_sum = dfm1.sum().sum()\n",
    "dfm1_row = dfm1.sum(axis=1)\n",
    "dfm1_col = dfm1.sum()\n",
    "\n",
    "print(f\"Summation of the Utility Matrix: {dfm1_sum}\")\n",
    "\n",
    "print(f\"Highest Row Sum: {np.max(dfm1_row)}\")\n",
    "\n",
    "print(f\"Highest Column Sum: {np.max(dfm1_col)}\") \n",
    "\n",
    "#train and test data\n",
    "train = dfm1.iloc[0:986,0:827]\n",
    "test = dfm1.iloc[986:1160,827:973]\n",
    "\n",
    "train_sum = train.sum().sum()\n",
    "print(f\"Summation of train matrix: {train_sum}\")\n",
    "\n",
    "print(f\"Dimension of test matrix: {test.shape}\")\n",
    "\n",
    "test_sum = test.sum().sum()\n",
    "print(f\"Summation Value of test matrix: {test_sum}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "268b7322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method | Rating Prediction Function | Metric | N=2 | N=3 | N=5\n",
      "------------------------------------------------------------\n",
      "Item-Item | Simple Average | RMSE | 0.9741 | 0.7659 | 0.8546 | \n",
      "Item-Item | Weighted Average | RMSE | 0.8251 | 0.7415 | 0.5467 | \n",
      "User-User | Simple Average | RMSE | 0.8530 | 0.7999 | 0.7451 | \n",
      "User-User | Weighted Average | RMSE | 0.6942 | 0.6782 | 0.6553 | \n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------question4--------------\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "# Calculate Item-Item Similarity Matrix\n",
    "def calculate_item_similarity(matrix):\n",
    "\n",
    "    return matrix.T.corr(method='pearson')\n",
    "    \n",
    "\n",
    "item_similarity_matrix = dfm1.corr(method='pearson')\n",
    "user_similarity_matrix = dfm1.T.corr(method='pearson')\n",
    "\n",
    "\n",
    "# Predict Rating for Item-Item (Simple Average)\n",
    "def predict_rating_simple_avg(user_id, tag_id, N):\n",
    "\n",
    "    if tag_id not in item_similarity_matrix.columns:\n",
    "        return 0\n",
    "\n",
    "    similar_tags = item_similarity_matrix[tag_id].nlargest(N+1).index[1:N+1]\n",
    "    ratings = dfm1.loc[user_id, similar_tags]\n",
    "\n",
    "    return ratings.mean() if not ratings.empty else 0\n",
    "\n",
    "# Predict Rating for Item-Item (Weighted Average)\n",
    "def predict_rating_weighted_avg(user_id, tag_id, N):\n",
    "\n",
    "    if tag_id not in item_similarity_matrix.columns:\n",
    "        return 0\n",
    "\n",
    "    similar_tags = item_similarity_matrix[tag_id].nlargest(N+1).index[1:N+1]\n",
    "    weights = item_similarity_matrix[tag_id][similar_tags]\n",
    "    \n",
    "    ratings = dfm1.loc[user_id, similar_tags]\n",
    "    valid_ratings = ratings.notnull()\n",
    "\n",
    "    weighted_sum = (ratings[valid_ratings] * weights[valid_ratings]).sum()\n",
    "    weight_sum = weights[valid_ratings].sum()\n",
    "\n",
    "    return weighted_sum / weight_sum if weight_sum != 0 else 0\n",
    "\n",
    "# Predict Rating for User-User (Simple Average)\n",
    "def predict_rating_simple_avg_user(user_id, tag_id, N):\n",
    "\n",
    "    if user_id not in user_similarity_matrix.index: # Check if user_id exists in the index\n",
    "        return 0\n",
    "\n",
    "    if tag_id not in dfm1.columns:\n",
    "        return 0\n",
    "\n",
    "    similar_users = user_similarity_matrix.loc[user_id].nlargest(N+1).index[1:N+1]\n",
    "    ratings = dfm1.loc[similar_users, tag_id]\n",
    "\n",
    "    return ratings.mean() if not ratings.empty else 0\n",
    "\n",
    "# Predict Rating for User-User (Weighted Average)\n",
    "def predict_rating_weighted_avg_user(user_id, tag_id, N):\n",
    "\n",
    "    if user_id not in user_similarity_matrix.index: # Check if user_id exists in the index\n",
    "        return 0\n",
    "\n",
    "    if tag_id not in dfm1.columns:\n",
    "        return 0\n",
    "\n",
    "    similar_users = user_similarity_matrix.loc[user_id].nlargest(N+1).index[1:N+1]\n",
    "    weights = user_similarity_matrix.loc[user_id, similar_users]\n",
    "\n",
    "    ratings = dfm1.loc[similar_users, tag_id]\n",
    "    valid_ratings = ratings.notnull()\n",
    "\n",
    "    weighted_sum = (ratings[valid_ratings] * weights[valid_ratings]).sum()\n",
    "    weight_sum = weights[valid_ratings].sum()\n",
    "\n",
    "    return weighted_sum / weight_sum if weight_sum != 0 else 0\n",
    "\n",
    "# Evaluate RMSE for User-User\n",
    "def evaluate_rmse_user(predict_function, N):\n",
    "    \n",
    "    actual_ratings = []\n",
    "    predicted_ratings = []\n",
    "\n",
    "    for user_id in test.index:\n",
    "\n",
    "        for tag_id in test.columns:\n",
    "\n",
    "            if not pd.isnull(test.loc[user_id, tag_id]):\n",
    "\n",
    "                actual_ratings.append(test.loc[user_id, tag_id])\n",
    "                predicted_ratings.append(predict_function(user_id, tag_id, N))\n",
    "\n",
    "     # Convert to numpy arrays for filtering NaNs\n",
    "    actual_ratings = np.array(actual_ratings)\n",
    "    predicted_ratings = np.array(predicted_ratings)\n",
    "\n",
    "    # Filter out any NaN values in the arrays\n",
    "    mask = ~np.isnan(actual_ratings) & ~np.isnan(predicted_ratings)\n",
    "    actual_ratings = actual_ratings[mask]\n",
    "    predicted_ratings = predicted_ratings[mask]\n",
    "\n",
    "    return np.sqrt(mean_squared_error(actual_ratings, predicted_ratings))\n",
    "\n",
    "\n",
    "# Evaluate RMSE for Item-Item\n",
    "def evaluate_rmse(predict_function, N):\n",
    "\n",
    "    actual_ratings = []\n",
    "    predicted_ratings = []\n",
    "\n",
    "    for user_id in test.index:\n",
    "\n",
    "        for tag_id in test.columns:\n",
    "\n",
    "            if not pd.isnull(test.loc[user_id, tag_id]):\n",
    "\n",
    "                actual_ratings.append(test.loc[user_id, tag_id])\n",
    "                predicted_ratings.append(predict_function(user_id, tag_id, N))\n",
    "\n",
    "\n",
    "    # Convert to numpy arrays for filtering NaNs\n",
    "    actual_ratings = np.array(actual_ratings)\n",
    "    predicted_ratings = np.array(predicted_ratings)\n",
    "\n",
    "    # Filter out any NaN values in the arrays\n",
    "    mask = ~np.isnan(actual_ratings) & ~np.isnan(predicted_ratings)\n",
    "    actual_ratings = actual_ratings[mask]\n",
    "    predicted_ratings = predicted_ratings[mask]\n",
    "\n",
    "    return np.sqrt(mean_squared_error(actual_ratings, predicted_ratings))\n",
    "\n",
    "def display_results():\n",
    "\n",
    "    N_values = [2, 3, 5]\n",
    "\n",
    "    print(\"Method | Rating Prediction Function | Metric | N=2 | N=3 | N=5\")\n",
    "    print(\"------------------------------------------------------------\")\n",
    "\n",
    "    lis = [(\"Item-Item\", predict_rating_simple_avg, predict_rating_weighted_avg) \n",
    "            ,(\"User-User\", predict_rating_simple_avg_user, predict_rating_weighted_avg_user)]\n",
    "\n",
    "\n",
    "    for method, predict_simple, predict_weighted in lis:\n",
    "\n",
    "        print(f\"{method} | Simple Average | RMSE |\", end=\" \")\n",
    "\n",
    "        for N in N_values:\n",
    "            rmse = evaluate_rmse(predict_simple, N) if method == \"Item-Item\" else evaluate_rmse_user(predict_simple, N)\n",
    "            print(f\"{rmse:.4f} |\", end=\" \")\n",
    "\n",
    "\n",
    "        print()\n",
    "\n",
    "        print(f\"{method} | Weighted Average | RMSE |\", end=\" \")\n",
    "\n",
    "        for N in N_values:\n",
    "            rmse = evaluate_rmse(predict_weighted, N) if method == \"Item-Item\" else evaluate_rmse_user(predict_weighted, N)\n",
    "            print(f\"{rmse:.4f} |\", end=\" \")\n",
    "\n",
    "        \n",
    "\n",
    "        print()\n",
    "\n",
    "display_results()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
